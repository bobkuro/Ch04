{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4章　ニューラルネットワークの学習\n",
    "\n",
    "## 4.1 \"データ駆動\"ならびに\"訓練データとテストデータ\"\n",
    "訓練データセットを元にテストデータセットの画像はどのようにラベリングすればよいか、正答率があがるのかという研究が行われてきた。<br>\n",
    "例えば手書き数字の認識があげられるだろう。<br>\n",
    "黎明期、人が手書き文字の癖を加味しながらその文字が\"\"何\"\"であることのルールを探していた。<br>\n",
    "頭からひねり出す代わりに機械に学習させる動きが出てきた。<br>\n",
    "画像から\"特徴量\"を抽出して、パターンを機械に学習させた。<br>\n",
    "この時点では\"特徴量\"は人の考えたものであった。<br>\n",
    "今では特徴量の設定から学習まで機械に投げるニューラルネットワークがあるわけだ。<br>\n",
    "この手法でも簡単にうまくいくわけではなく、過学習のような問題が累積している。<br>\n",
    "\n",
    "## 4.2 損失関数\n",
    "過学習が起きる話は個人的に何度か触れているのでわかっているとは思うが、<br>\n",
    "任意の答えを出力する関数を求める為に多くの観点から学習させて精度を上げたいところだが、目的のものよりも必要以上に複雑になってしまうこと。<br>\n",
    "(本来はsin関数で済むところが10次関数くらいになっててオイオイといった感じの。)<br>\n",
    "![](ow.jpg\")\n",
    "\n",
    "\n",
    "### 4.2.1 2乗和誤差\n",
    "損失関数として有名なのは２乗誤差である。以下に数式を示す。くコ:彡<br>\n",
    "$$E= \\frac {1} {2\n",
    "} \\sum _k (y_k - t_k)^2$$\n",
    "上式のykはNNの出力でありソレに対応した確率、tkは教師データであり答えを指す。<br>\n",
    "MNISTのデータセットでうんにゃらした時を思い出しながら例を示す。<br>\n",
    "yとtは10個の要素からなるデータであり、2であることを判定したい。<br>\n",
    "2乗和誤差を定義してぐだぐだ言わずに突っ込んでみる。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    ">>> t=[0,0,1,0,0,0,0,0,0,0]\n",
    ">>> def mean_squared_error(y,t):\n",
    "...     return 0.5  * np.sum((y-t)**2)\n",
    ">>> mean_squared_error(np.array(y),np.array(t))\n",
    "0.097500000000000031\n",
    ">>> y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.5,0.0]\n",
    ">>> mean_squared_error(np.array(y),np.array(t))\n",
    "0.72250000000000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と、このように<br>\n",
    "yの要素の中で2を1番高い確率にした場合と、7を1番高い確率にした場合でエラー率が大きく違うことがわかるだろう。<br>\n",
    "よって1つ目の例のほうが教師データに適合している<br>\n",
    "\n",
    "### 4.2.2 交差エントロピー誤差\n",
    "以下に数式を示す。くコ:彡\n",
    "$$E=- \\sum _k t_k log y_k $$\n",
    "yとtは4.2.1と同じなので省略<br>\n",
    "コレの良い点は正解ラベルとなる出力の結果を上式に突っ込むだけで誤差が分かるということにあることであろう。<br>\n",
    "グラフより逆算がしやすく、感覚的にわかりやすいね。やったね。<br>\n",
    "同じように示す。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    ">>> def cross_entropy_error(y,t):\n",
    "...     delta = 1e-7\n",
    "...     return -np.sum(t * np.log(y+delta))\n",
    ">>> cross_entropy_error(np.array(y),np.array(t))\n",
    "0.51082545709933802\n",
    ">>> cross_entropy_error(np.array(y),np.array(t))\n",
    "2.3025840929945458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ここまでは此の中でプログラムを走らせることを忘れていたことを許して欲しい。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 ミニバッチ学習\n",
    "これまでは１つのデータに対しての損失関数の話をしてきましたここからは全てのデータに対して行う。<br>\n",
    "N個のデータ用に拡張しただけなので式は省略する。<br>\n",
    "MNISTはご存知の通り６万ものデータがあり、その全てから学習するのはコストがかかる。ここで訓練データからデータの一部を取り出して近似的に学習をする。これをミニバッチ学習と言う。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train,t_train), (x_test, t_test) = \\\n",
    "    load_mnist( normalize = True, one_hot_label=True)\n",
    "    \n",
    "print(x_train.shape) \n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "６万の28*28のデータとワンホットラベルを作れたのがわかった。<br>次にバッチ学習の為に適当に取り出す画像を選ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27207, 24440, 37582, 58044, 49885, 25599, 47588,  9344, 22097, 43232])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ対応版の交差エントロピーの実装を以下に記述する。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.reshape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size\n",
    "#    return -np.sum(np.log(y[np.arrange(batch_size),t])) / batch_size ワンホットラベルでなく数字ラベルの場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 数値微分\n",
    "### 4.3.1 微分\n",
    "軽く微分の復習を。数値微分の定義と言えば以下である。くコ:彡\n",
    "    $$ \\frac {df(x)}{dx} = lim_{h \\rightarrow 0} \\frac {f(x+h)+f(x)} {h} $$\n",
    "pythonでは丸め誤差があることを念頭に置いて、微小な値hは10の-4乗程度を使うと良い結果が得られることがわかっている。<br>\n",
    "また、xとx+hから求めた接線であるので、実は真の接線ではない。<br>ここで(x+h)と(x-h)での関数fの差分を調べることで解消することにする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h= 1e-4\n",
    "    return (f(x+h) - f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 数値微分の例\n",
    "以下のような簡単な関数を微分して精度を確かめてみる。くコ:彡\n",
    "$$ y=0.01x^2 + 0.1x $$\n",
    "Pythonで実装したものが以下である。くコ:彡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.arange(0.0, 20.0, 0.1)\n",
    "y=function_1(x)\n",
    "\n",
    "numerical_diff(function_1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10の-13乗レベルで誤差が小さい！すごい！\n",
    "\n",
    "### 4.3.3 偏微分\n",
    "続いて以下のような数式について見ていく。<br>\n",
    "$$ f(x_{0} , x_{1}) =x_{0}^2 +x_{1}^2 $$\n",
    "Pythonでは以下の様に書ける。くコ:彡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下からはx0=3,x1=4のときのそれぞれの微分について求める。くコ:彡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp0(x0): #x0に対する偏微分を求める \n",
    "    return x0*x0+4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp0,3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x1): #x1に対する偏微分を求める \n",
    "    return 3.0**2.0+x1*x1\n",
    "\n",
    "numerical_diff(function_tmp0,4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように複数の変数があっても傾きは求められるよということだ。\n",
    "\n",
    "## 4.4 勾配\n",
    "先程は2変数についてそれぞれ計算をした。<br>ここからはまとめて計算したいご要望にお答えする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx]=tmp_val+h\n",
    "        fxh1=f(x)\n",
    "\n",
    "        x[idx]=tmp_val-h\n",
    "        fxh2=f(x)\n",
    "\n",
    "        grad[idx] = (fxh1- fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  8.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やったあぁ！さっきの(6.00000000000378,7.999999999999119)とちっかいゾ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6., -8.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([-3.0,-4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配がマイナスのこともある。<br>これですり鉢状のfunction_2の最小値を探ることが出来るのがわかるだろう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 勾配法\n",
    "先程まで勾配が…とか傾きがマイナスが…と少しずつ述べてきたがここから手法として本格的に考えていく。以下に勾配法の数式を表す。くコ:彡\n",
    "$$ x_{0}=x_{0} - \\eta \\frac{\\partial f}{\\partial x_{0}}  $$\n",
    "ηは更新の量を表し、学習率と呼ばれる。<br>1回の学習でどれだけ学習とパラメータの更新をするかをきめる重要なファクターである。\n",
    "\n",
    "fは最適化したい関数、init_xは初期値、lrは学習率、step_numを勾配法の繰り返し回数とする。<br>関数の傾きを求めて以下にPythonで実装したものを示す。くコ:彡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x=init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期位置を(-3.0,4.0)として勾配法を用いて探索をして見る。<br>\n",
    "引数を見てくれればどんな感じかは察せると思う。<br>以下に成功例、学習率が大きすぎる場合、小さすぎる場合についてそれぞれ例を出す。くコ:彡 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.58983747e+13,  -1.29524862e+12])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 NNに対する勾配\n",
    "ここで2×3の形状の重みWだけ持つNNがある。<br>損失関数をLで表す場合を考える。\n",
    "<br>行列で表すとWと損失関数の変化を表す行列は以下のように表せる。くコ:彡\n",
    "$$\n",
    "  W = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      W_{11} & W_{21} & W_{31} \\\\\n",
    "      W_{12} & W_{22} & W_{32} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$\n",
    "$$\n",
    "  \\frac{\\delta L}{\\delta W} = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      \\frac{\\delta L}{\\delta w_{11}} & \\frac{\\delta L}{\\delta w_{21}} & \\frac{\\delta L}{\\delta w_{31}} \\\\\n",
    "      \\frac{\\delta L}{\\delta w_{12}} & \\frac{\\delta L}{\\delta w_{22}} & \\frac{\\delta L}{\\delta w_{32}} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.rand(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44936032  0.26360891  0.17456213]\n",
      " [ 0.37696559  0.97913008  0.20215002]]\n"
     ]
    }
   ],
   "source": [
    "net =simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.60888522  1.03938242  0.2866723 ]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([0.6,0.9])\n",
    "p=net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1825139802000499"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.array([1,0,0])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75201678553279705"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.array([0,1,0])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5047269085428816"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.array([0,0,1])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2225752   0.2094516  -0.43202679]\n",
      " [ 0.33386279  0.3141774  -0.64804019]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "dW = numerical_gradient(f,net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(f,net.W)の結果はdwとなり、形状が2×3の二次元配列となる。<br>\n",
    "dWの中身を見ると1行1列目の値hogeから、Wの値をhだけ増やすと、損失関数の値がh*hogeの分だけ増加することを意味する。<br>\n",
    "また、マイナスの項目を見ると、損失関数が減少する。<br>\n",
    "損失関数を減少させる観点おいてマイナスの項目の方が大きく貢献しているのだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 学習アルゴリズムの実行\n",
    "ここまで駆け足でしたが、損失関数、ミニバッチ、勾配、勾配降下法について勉強してきました。<br>これらを用いてNNの学習手順をまとめて行きます。\n",
    "#### 前提\n",
    "ニューラルネットワークは、適応可能な重みとバイアスがありこの２つを訓練データに適応するように調整することを学習と呼ぶ。\n",
    "#### ステップ１ ミニバッチ\n",
    "訓練データの中からランダムに一部のデータを選び出す。その選ばれたデータをミニバッチと言う。ここではそのミニバッチの損失関数の値を減らすことを目的とする。\n",
    "#### ステップ２ 勾配の算出\n",
    "ミニバッチの損失関数を減らす為に、各重みのパラメータの勾配を求める。勾配は、損失関数の値を最も減らす方向を示す。\n",
    "#### ステップ３ パラメータの更新\n",
    "重みパラメータを勾配方向に微小だが更新する\n",
    "#### ステップ４ 繰り返す\n",
    "<br><br>\n",
    "\n",
    "この手法は確率的勾配降下法と呼ばれている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "train acc, test acc | 0.0993, 0.1032\n",
      "train acc, test acc | 0.790833333333, 0.793\n",
      "train acc, test acc | 0.878166666667, 0.8827\n",
      "train acc, test acc | 0.899766666667, 0.9036\n",
      "train acc, test acc | 0.908733333333, 0.9108\n",
      "train acc, test acc | 0.9154, 0.9161\n",
      "train acc, test acc | 0.91985, 0.9207\n",
      "train acc, test acc | 0.923233333333, 0.9255\n",
      "train acc, test acc | 0.926783333333, 0.9293\n",
      "train acc, test acc | 0.930583333333, 0.9322\n",
      "train acc, test acc | 0.9327, 0.9342\n",
      "train acc, test acc | 0.936266666667, 0.9363\n",
      "train acc, test acc | 0.9392, 0.9395\n",
      "train acc, test acc | 0.940666666667, 0.94\n",
      "train acc, test acc | 0.94245, 0.9405\n",
      "train acc, test acc | 0.944633333333, 0.9427\n",
      "train acc, test acc | 0.946616666667, 0.9465\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNUylAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b318978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000  # 繰り返しの回数を適宜設定する\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
